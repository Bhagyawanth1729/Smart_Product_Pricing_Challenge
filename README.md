ğŸ§  Project Title:

Smart Product Pricing using Supervised Autoencoder + Stacking Ensemble

ğŸ“‹ Problem Statement

E-commerce platforms rely heavily on accurate price prediction for optimal competitiveness and profitability. The goal of this challenge is to predict the price of products using their textual descriptions, image representations, and structured attributes â€” without relying on any external pricing data.

ğŸ§° Approach Overview

This solution integrates deep learning feature extraction and traditional machine learning regression using a stacking ensemble.

Key Idea:

Combine multiple data modalities â€” text, image, and structured product attributes â€” into a unified predictive model via:

Supervised Autoencoder (for learning compact image embeddings)

Stacked Ensemble of LightGBM, XGBoost, and MLP models

LightGBM Meta-Model trained on out-of-fold predictions

ğŸ—ï¸ Model Architecture
1ï¸âƒ£ Feature Engineering
Feature Type	Description	Technique
Structured Features	Product metadata, item quantity, etc.	StandardScaler normalization
Text Features	Combined title, description, and item pack quantity	Pre-trained text embeddings (384-dim) + StandardScaler
Image Features	CNN embeddings extracted from product images	Compressed using a Supervised Autoencoder to 512-dim

All three feature types are concatenated â†’ 901-dim final feature vector per product.

2ï¸âƒ£ Supervised Autoencoder (Image Feature Compression)

Encoder: 2048 â†’ 1024 â†’ 512

Decoder: 512 â†’ 1024 â†’ 2048

Regression Head: 512 â†’ 1

Loss Function:

Total Loss = 0.5 * MSE(reconstruction) + 0.5 * MSE(price prediction)


Training: 50 epochs, batch size 256, Adam optimizer (lr = 1e-3)

âœ… Final AE loss reduced from 0.63 â†’ 0.116 (excellent convergence)

3ï¸âƒ£ Stacking Ensemble (Supervised Learning Stage)

Base Models:

LightGBM Regressor

XGBoost Regressor

MLP Regressor

Meta-Model:

LightGBM Regressor trained on 5-fold OOF (out-of-fold) predictions.

âš™ï¸ Training Setup
Component	Setting
Frameworks	PyTorch, scikit-learn, LightGBM, XGBoost
Hardware	Colab GPU (T4)
Data Size	75,000 products
Loss Metrics	RMSE, RÂ², SMAPE
Random Seed	42
ğŸ“Š Performance Summary (Training Results)
Metric	Value	Interpretation
RMSE (Meta-Model)	0.2758	Low log-space error
RÂ² Score	0.9143	Explains 91% of price variance
SMAPE	21.74 %	Strong performance for e-commerce price prediction
ğŸ“¦ Files Generated
File	Description
stacked_model_supervised_autoencoder.pkl	Saved full model (scalers, autoencoder weights, base models, meta model)
test_out.csv	Final predicted prices for submission
test_pca_transformer.pkl (optional)	PCA reducer (if used for dimensionality reduction)
ğŸ§© Testing / Inference Pipeline

Load saved model and scalers

Scale structured, text, and image features from test data

Pass image features through trained autoencoder encoder

Concatenate all features

Generate base-model predictions â†’ feed to meta-model

Predict final prices â†’ save as test_out.csv

ğŸ§® Evaluation Metric

The model is evaluated using SMAPE (Symmetric Mean Absolute Percentage Error):

ğ‘†
ğ‘€
ğ´
ğ‘ƒ
ğ¸
=
1
ğ‘›
âˆ‘
âˆ£
ğ‘¦
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘‘
âˆ’
ğ‘¦
ğ‘¡
ğ‘Ÿ
ğ‘¢
ğ‘’
âˆ£
(
âˆ£
ğ‘¦
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘‘
âˆ£
+
âˆ£
ğ‘¦
ğ‘¡
ğ‘Ÿ
ğ‘¢
ğ‘’
âˆ£
)
/
2
Ã—
100
SMAPE=
n
1
	â€‹

âˆ‘
(âˆ£y
pred
	â€‹

âˆ£+âˆ£y
true
	â€‹

âˆ£)/2
âˆ£y
pred
	â€‹

âˆ’y
true
	â€‹

âˆ£
	â€‹

Ã—100

Lower SMAPE indicates better performance.

ğŸš€ Conclusion

âœ… The Supervised Autoencoder + Stacking Ensemble approach achieved:

Strong generalization (RÂ² â‰ˆ 0.91)

Competitive SMAPE (â‰ˆ 21.7%)

Excellent balance of interpretability and predictive power.

This performance is acceptable and submission-ready for the ML Challenge 2025 leaderboard.

ğŸ‘¨â€ğŸ’» Author

Bhagyawanth
B.Tech Computer Science | Data Analyst & ML Enthusiast
ğŸ“§ Email: [your email here]
ğŸ§  Tools: Python, PyTorch, scikit-learn, LightGBM, XGBoost
