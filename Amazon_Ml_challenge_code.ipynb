{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# importing packages"
      ],
      "metadata": {
        "id": "EZXhJyQMOMrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2YGyBTuA1Wv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train CSV File"
      ],
      "metadata": {
        "id": "wT70yXImOZo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "yTjKyu9xBn2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pretrained ResNet50 and remove the last classification\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FS_FMfGHOiSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained ResNet50 and remove the last classification layer\n",
        "model = models.resnet50(pretrained=True)\n",
        "model = torch.nn.Sequential(*(list(model.children())[:-1]))  # output = 2048-dim\n",
        "model.eval()\n",
        "\n",
        "# Define image transform (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCzqgIJ4BrRS",
        "outputId": "72dcab54-797d-477e-a5c6-0fe2063da7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 394MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Image_Feature Extraction function"
      ],
      "metadata": {
        "id": "JloG7iCDO3eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image_feature(img_url):\n",
        "    try:\n",
        "        response = requests.get(img_url, timeout=5)\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        img_t = transform(img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            feat = model(img_t).squeeze().numpy()\n",
        "        return feat\n",
        "    except Exception:\n",
        "        return np.zeros(2048)  # fallback if image fails\n"
      ],
      "metadata": {
        "id": "e81dwU5sBuO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Feature storing and Extracting from image \"Url'"
      ],
      "metadata": {
        "id": "31FJKOO-PBUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = []\n",
        "\n",
        "for url in tqdm(train_df['image_link'], desc=\"Extracting train image features\"):\n",
        "    feat = extract_image_feature(url)\n",
        "    image_features.append(feat)\n",
        "\n",
        "image_features = np.array(image_features)\n",
        "print(\"Train image features shape:\", image_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq5RZgukBys9",
        "outputId": "6f8272db-6632-430e-af99-3767a20e92f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting train image features:  41%|████      | 30539/75000 [1:26:27<1:53:39,  6.52it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save as .npy file,.csv file for model training\n"
      ],
      "metadata": {
        "id": "Q_FW9AeAPOIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('train_image_features.npy', image_features)\n",
        "\n",
        "# (Optional) Save as CSV if you want to merge later\n",
        "df = pd.DataFrame(image_features)\n",
        "df['sample_id'] = train_df['sample_id']\n",
        "df.to_csv('train_image_feature.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ZhiSIks1B3OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AUTOMATIC SEMANTIC EMBEDDING & CATEGORIZATION PIPELINE\n"
      ],
      "metadata": {
        "id": "H4UBzhL7Py0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install -q sentence-transformers umap-learn hdbscan plotly matplotlib seaborn pandas numpy torch tqdm scikit-learn nltk\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "import hdbscan\n",
        "from collections import Counter\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# TEXT CLEANING FUNCTION\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean product descriptions for better semantic clustering\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"no description\"\n",
        "\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "\n",
        "    # Remove special characters but keep meaningful punctuation\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s\\-\\.]\", \" \", text)\n",
        "\n",
        "    # Remove common e-commerce stop words\n",
        "    ecommerce_stop_words = {\n",
        "        'bullet', 'point', 'item', 'name', 'value', 'product', 'com', 'description',\n",
        "        'feature', 'benefit', 'specification', 'detail', 'information', 'please',\n",
        "        'buy', 'purchase', 'order', 'shipping', 'delivery', 'price', 'sale',\n",
        "        'brand', 'new', 'free', 'best', 'quality', 'high', 'premium'\n",
        "    }\n",
        "\n",
        "    # Tokenize and remove stop words\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in ecommerce_stop_words and len(w) > 2]\n",
        "\n",
        "    text = \" \".join(words)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text if text else \"no description\"\n",
        "\n",
        "# EMBEDDING EXTRACTOR\n",
        "\n",
        "class SemanticEmbedder:\n",
        "    def _init_(self, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "        print(f\" Loading model: {model_name}\")\n",
        "        try:\n",
        "            # Using a larger model for better semantic understanding\n",
        "            self.model = SentenceTransformer(model_name)\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.model = self.model.to(self.device)\n",
        "            print(f\" Using device: {self.device}\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading model: {e}\")\n",
        "            # Fallback to smaller model\n",
        "            self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "            self.device = \"cpu\"\n",
        "            self.model = self.model.to(self.device)\n",
        "\n",
        "    def get_embeddings(self, texts, batch_size=32):\n",
        "        \"\"\"Generate embeddings for a list of texts\"\"\"\n",
        "        print(f\" Processing {len(texts)} texts...\")\n",
        "\n",
        "        # Clean texts with progress bar\n",
        "        cleaned_texts = []\n",
        "        for text in tqdm(texts, desc=\"Cleaning texts\"):\n",
        "            cleaned_texts.append(clean_text(text))\n",
        "\n",
        "        print(\" Generating embeddings...\")\n",
        "        try:\n",
        "            embeddings = self.model.encode(\n",
        "                cleaned_texts,\n",
        "                batch_size=batch_size,\n",
        "                show_progress_bar=True,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True,\n",
        "                device=self.device\n",
        "            )\n",
        "            print(f\" Generated embeddings shape: {embeddings.shape}\")\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            print(f\" Error generating embeddings: {e}\")\n",
        "            raise\n",
        "\n",
        "# CATEGORIZATION WITH BETTER CLUSTERING\n",
        "class ImprovedAutoCategorizer:\n",
        "    def _init_(self, dataset_size=None):\n",
        "        self.dataset_size = dataset_size\n",
        "\n",
        "        # More conservative UMAP parameters for better separation\n",
        "        self.umap_params = {\n",
        "            'n_neighbors': min(20, max(10, dataset_size // 50)) if dataset_size else 15,\n",
        "            'n_components': min(50, max(15, dataset_size // 20)) if dataset_size else 25,\n",
        "            'min_dist': 0.05,\n",
        "            'metric': \"cosine\",\n",
        "            'random_state': 42,\n",
        "            'low_memory': False\n",
        "        }\n",
        "\n",
        "        # More conservative HDBSCAN parameters\n",
        "        if dataset_size:\n",
        "            min_cluster_size = max(10, dataset_size // 100)\n",
        "            min_samples = max(5, dataset_size // 200)        5\n",
        "        else:\n",
        "            min_cluster_size = 15\n",
        "            min_samples = 5\n",
        "\n",
        "        self.hdbscan_params = {\n",
        "            'min_cluster_size': min_cluster_size,\n",
        "            'min_samples': min_samples,\n",
        "            'cluster_selection_epsilon': 0.1,\n",
        "            'metric': \"euclidean\",\n",
        "            'cluster_selection_method': 'eom',\n",
        "            'prediction_data': True\n",
        "        }\n",
        "\n",
        "        self.umap_reducer = None\n",
        "        self.clusterer = None\n",
        "        self.labels_ = None\n",
        "        self.embedding_2d = None\n",
        "        self.category_interpretations = {}\n",
        "        self.nn_classifier = None\n",
        "        self.cluster_centroids = None\n",
        "\n",
        "        print(f\" Improved parameters for dataset size {dataset_size}:\")\n",
        "        print(f\"   - min_cluster_size: {self.hdbscan_params['min_cluster_size']}\")\n",
        "        print(f\"   - min_samples: {self.hdbscan_params['min_samples']}\")\n",
        "\n",
        "    def fit(self, embeddings, product_descriptions=None):\n",
        "        \"\"\"Fit the categorization model with improved clustering\"\"\"\n",
        "        print(\" Starting Improved Automatic Categorization\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Step 1: UMAP with multiple attempts if needed\n",
        "        print(\" Step 1: UMAP Dimensionality Reduction\")\n",
        "        best_embeddings_reduced = None\n",
        "        best_score = -1\n",
        "\n",
        "        # Try different UMAP parameters to find best separation\n",
        "        umap_configs = [\n",
        "            self.umap_params,\n",
        "            {**self.umap_params, 'min_dist': 0.01, 'n_neighbors': 15},\n",
        "            {**self.umap_params, 'min_dist': 0.1, 'n_neighbors': 25}\n",
        "        ]\n",
        "\n",
        "        for i, config in enumerate(umap_configs):\n",
        "            print(f\"   - Trying UMAP config {i+1}: n_neighbors={config['n_neighbors']}, min_dist={config['min_dist']}\")\n",
        "            try:\n",
        "                umap_reducer = umap.UMAP(**config)\n",
        "                embeddings_reduced = umap_reducer.fit_transform(embeddings)\n",
        "\n",
        "                # Quick clustering to evaluate this configuration\n",
        "                test_clusterer = hdbscan.HDBSCAN(\n",
        "                    min_cluster_size=self.hdbscan_params['min_cluster_size'],\n",
        "                    min_samples=self.hdbscan_params['min_samples']\n",
        "                )\n",
        "                test_labels = test_clusterer.fit_predict(embeddings_reduced)\n",
        "\n",
        "                # Score based on number of clusters and noise ratio\n",
        "                n_clusters = len(set(test_labels)) - (1 if -1 in test_labels else 0)\n",
        "                noise_ratio = np.sum(test_labels == -1) / len(test_labels)\n",
        "\n",
        "                if n_clusters > 0:\n",
        "                    score = n_clusters * (1 - noise_ratio)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_embeddings_reduced = embeddings_reduced\n",
        "                        self.umap_reducer = umap_reducer\n",
        "                        print(f\"      Good configuration: {n_clusters} clusters, {noise_ratio:.1%} noise\")\n",
        "            except Exception as e:\n",
        "                print(f\"      UMAP config {i+1} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        if best_embeddings_reduced is None:\n",
        "            print(\" All UMAP configurations failed, using default\")\n",
        "            self.umap_reducer = umap.UMAP(**self.umap_params)\n",
        "            best_embeddings_reduced = self.umap_reducer.fit_transform(embeddings)\n",
        "\n",
        "        embeddings_reduced = best_embeddings_reduced\n",
        "        print(f\"Final reduced embeddings shape: {embeddings_reduced.shape}\")\n",
        "\n",
        "        # Step 2:HDBSCAN with multiple attempts\n",
        "        print(\"\\n Step 2: HDBSCAN Clustering\")\n",
        "        best_labels = None\n",
        "        best_clusterer = None\n",
        "        best_silhouette = -1\n",
        "\n",
        "        hdbscan_configs = [\n",
        "            self.hdbscan_params,\n",
        "            {**self.hdbscan_params, 'min_cluster_size': max(5, self.dataset_size // 200)},\n",
        "            {**self.hdbscan_params, 'cluster_selection_epsilon': 0.05}\n",
        "        ]\n",
        "\n",
        "        for i, config in enumerate(hdbscan_configs):\n",
        "            print(f\"   - Trying HDBSCAN config {i+1}: min_cluster_size={config['min_cluster_size']}\")\n",
        "            try:\n",
        "                clusterer = hdbscan.HDBSCAN(**config)\n",
        "                labels = clusterer.fit_predict(embeddings_reduced)\n",
        "\n",
        "                # Calculate quality metrics\n",
        "                non_noise_mask = labels != -1\n",
        "                non_noise_labels = labels[non_noise_mask]\n",
        "\n",
        "                if len(np.unique(non_noise_labels)) >= 2:\n",
        "                    non_noise_embeddings = embeddings_reduced[non_noise_mask]\n",
        "                    silhouette = silhouette_score(non_noise_embeddings, non_noise_labels)\n",
        "\n",
        "                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "                    noise_ratio = np.sum(labels == -1) / len(labels)\n",
        "\n",
        "                    print(f\"      {n_clusters} clusters, noise: {noise_ratio:.1%}, silhouette: {silhouette:.3f}\")\n",
        "\n",
        "                    if silhouette > best_silhouette and n_clusters >= 2:\n",
        "                        best_silhouette = silhouette\n",
        "                        best_labels = labels\n",
        "                        best_clusterer = clusterer\n",
        "                else:\n",
        "                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "                    print(f\"     Only {n_clusters} clusters found\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" HDBSCAN config {i+1} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        if best_labels is None:\n",
        "            print(\"All HDBSCAN configurations failed, using last attempt\")\n",
        "            self.clusterer = hdbscan.HDBSCAN(**self.hdbscan_params)\n",
        "            self.labels_ = self.clusterer.fit_predict(embeddings_reduced)\n",
        "        else:\n",
        "            self.clusterer = best_clusterer\n",
        "            self.labels_ = best_labels\n",
        "\n",
        "        # Final cluster statistics\n",
        "        n_clusters = len(set(self.labels_)) - (1 if -1 in self.labels_ else 0)\n",
        "        n_noise = np.sum(self.labels_ == -1)\n",
        "        coverage = (len(self.labels_) - n_noise) / len(self.labels_) * 100\n",
        "\n",
        "        print(f\"\\n FINAL CLUSTERING RESULTS:\")\n",
        "        print(f\"   - Clusters found: {n_clusters}\")\n",
        "        print(f\"   - Noise points: {n_noise} ({n_noise/len(self.labels_)*100:.1f}%)\")\n",
        "        print(f\"   - Coverage: {coverage:.1f}%\")\n",
        "\n",
        "        # Step 3: Train classifier\n",
        "        print(\"\\n Step 3: Training Classifier\")\n",
        "        self._train_nn_classifier(embeddings_reduced)\n",
        "\n",
        "        # Step 4: 2D visualization\n",
        "        print(\"\\n Step 4: Generating 2D Visualization\")\n",
        "        try:\n",
        "            umap_2d = umap.UMAP(n_components=2, random_state=42, metric='cosine', min_dist=0.1)\n",
        "            self.embedding_2d = umap_2d.fit_transform(embeddings)\n",
        "            print(\" 2D coordinates generated\")\n",
        "        except Exception as e:\n",
        "            print(f\" 2D visualization failed: {e}\")\n",
        "\n",
        "        # Step 5: Improved interpretations\n",
        "        if product_descriptions is not None and len(np.unique(self.labels_)) > 1:\n",
        "            print(\"\\n Step 5: Generating Improved Category Interpretations\")\n",
        "            try:\n",
        "                self._generate_improved_interpretations(embeddings, product_descriptions)\n",
        "                print(\" Category interpretations generated\")\n",
        "            except Exception as e:\n",
        "                print(f\" Interpretation generation failed: {e}\")\n",
        "        else:\n",
        "            print(\"\\n Step 5: Skipping interpretations\")\n",
        "\n",
        "        return self.labels_\n",
        "\n",
        "    def _train_nn_classifier(self, embeddings_reduced):\n",
        "        \"\"\"Train a nearest neighbor classifier\"\"\"\n",
        "        non_noise_mask = self.labels_ != -1\n",
        "        non_noise_embeddings = embeddings_reduced[non_noise_mask]\n",
        "        non_noise_labels = self.labels_[non_noise_mask]\n",
        "\n",
        "        if len(non_noise_labels) == 0:\n",
        "            print(\" No non-noise clusters found for NN classifier\")\n",
        "            self.nn_classifier = None\n",
        "            return\n",
        "\n",
        "        # Calculate cluster centroids\n",
        "        self.cluster_centroids = {}\n",
        "        unique_labels = np.unique(non_noise_labels)\n",
        "        for label in unique_labels:\n",
        "            mask = non_noise_labels == label\n",
        "            cluster_points = non_noise_embeddings[mask]\n",
        "            self.cluster_centroids[label] = np.mean(cluster_points, axis=0)\n",
        "\n",
        "        # Train KNN classifier\n",
        "        self.nn_classifier = NearestNeighbors(n_neighbors=5, metric='euclidean')\n",
        "        self.nn_classifier.fit(non_noise_embeddings)\n",
        "        self.nn_embeddings = non_noise_embeddings\n",
        "        self.nn_labels = non_noise_labels\n",
        "\n",
        "        print(f\" Trained NN classifier on {len(non_noise_labels)} non-noise samples\")\n",
        "\n",
        "    def _generate_improved_interpretations(self, embeddings, descriptions):\n",
        "        \"\"\"Generate meaningful category interpretations\"\"\"\n",
        "        unique_labels = np.unique(self.labels_)\n",
        "        if -1 in unique_labels:\n",
        "            unique_labels = unique_labels[unique_labels != -1]\n",
        "\n",
        "        print(f\" Analyzing {len(unique_labels)} categories...\")\n",
        "\n",
        "        # Extended stop words\n",
        "        extended_stop_words = {\n",
        "            \"the\", \"and\", \"for\", \"with\", \"this\", \"that\", \"are\", \"from\", \"has\", \"have\",\n",
        "            \"product\", \"description\", \"item\", \"name\", \"value\", \"bullet\", \"point\", \"com\",\n",
        "            \"your\", \"our\", \"will\", \"can\", \"one\", \"using\", \"use\", \"made\", \"make\", \"includes\",\n",
        "            \"include\", \"features\", \"feature\", \"benefits\", \"benefit\", \"specifications\"\n",
        "        }\n",
        "\n",
        "        for label in unique_labels:\n",
        "            mask = self.labels_ == label\n",
        "            category_embeddings = embeddings[mask]\n",
        "\n",
        "            if hasattr(descriptions, 'iloc'):\n",
        "                category_desc = descriptions.iloc[np.where(mask)[0]]\n",
        "            else:\n",
        "                category_desc = [descriptions[i] for i in np.where(mask)[0]]\n",
        "\n",
        "            if len(category_embeddings) == 0:\n",
        "                continue\n",
        "\n",
        "            # Find representative product (closest to centroid)\n",
        "            centroid = np.mean(category_embeddings, axis=0)\n",
        "            distances = np.linalg.norm(category_embeddings - centroid, axis=1)\n",
        "            rep_idx = np.argmin(distances)\n",
        "\n",
        "            # Extract meaningful keywords\n",
        "            all_text = \" \".join([str(desc) for desc in category_desc])\n",
        "\n",
        "            # Use multiple techniques to extract keywords\n",
        "            words = re.findall(r\"\\b[a-zA-Z]{3,15}\\b\", all_text.lower())\n",
        "            words = [w for w in words if w not in extended_stop_words and not w.isdigit()]\n",
        "\n",
        "            # Get frequency-based keywords\n",
        "            word_freq = Counter(words)\n",
        "            common_words = [w for w, count in word_freq.most_common(10) if count >= max(2, len(category_desc) * 0.1)]\n",
        "\n",
        "            # Get representative description\n",
        "            if hasattr(category_desc, 'iloc'):\n",
        "                rep_desc = category_desc.iloc[rep_idx]\n",
        "            else:\n",
        "                rep_desc = category_desc[rep_idx]\n",
        "\n",
        "            # Create a meaningful category name from top keywords\n",
        "            category_name = \" & \".join(common_words[:3]).title() if common_words else f\"Category {label}\"\n",
        "\n",
        "            short_desc = str(rep_desc)\n",
        "            if len(short_desc) > 150:\n",
        "                short_desc = short_desc[:147] + \"...\"\n",
        "\n",
        "            self.category_interpretations[label] = {\n",
        "                \"size\": len(category_embeddings),\n",
        "                \"category_name\": category_name,\n",
        "                \"representative_description\": short_desc,\n",
        "                \"top_keywords\": common_words[:5],\n",
        "                \"sample_size\": len(category_embeddings)\n",
        "            }\n",
        "\n",
        "    def predict(self, embeddings, method='hybrid'):\n",
        "        if self.umap_reducer is None or self.clusterer is None:\n",
        "            raise ValueError(\"Model must be fitted before prediction\")\n",
        "\n",
        "        unique_labels = np.unique(self.labels_)\n",
        "        non_noise_labels = unique_labels[unique_labels != -1]\n",
        "\n",
        "        if len(non_noise_labels) == 0:\n",
        "            print(\" No clusters found in training, returning all noise labels\")\n",
        "            return np.array([-1] * len(embeddings))\n",
        "\n",
        "        print(f\" Predicting categories for {len(embeddings)} samples using {method} method...\")\n",
        "\n",
        "        try:\n",
        "            embeddings_reduced = self.umap_reducer.transform(embeddings)\n",
        "\n",
        "            if method == 'hdbscan':\n",
        "                return self._predict_hdbscan(embeddings_reduced)\n",
        "            elif method == 'nearest_neighbor':\n",
        "                return self._predict_nearest_neighbor(embeddings_reduced)\n",
        "            elif method == 'centroid':\n",
        "                return self._predict_centroid(embeddings_reduced)\n",
        "            elif method == 'hybrid':\n",
        "                return self._predict_hybrid(embeddings_reduced)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown prediction method: {method}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Prediction failed: {e}\")\n",
        "            return np.array([-1] * len(embeddings))\n",
        "\n",
        "    def _predict_hdbscan(self, embeddings_reduced):\n",
        "        \"\"\"Use HDBSCAN's approximate_predict method\"\"\"\n",
        "        try:\n",
        "            labels, strengths = hdbscan.approximate_predict(self.clusterer, embeddings_reduced)\n",
        "            min_strength = 0.3\n",
        "            labels[strengths < min_strength] = -1\n",
        "\n",
        "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            print(f\"HDBSCAN prediction: {n_clusters} clusters found\")\n",
        "            return labels\n",
        "        except Exception as e:\n",
        "            print(f\" HDBSCAN prediction failed: {e}\")\n",
        "            return np.array([-1] * len(embeddings_reduced))\n",
        "\n",
        "    def _predict_nearest_neighbor(self, embeddings_reduced):\n",
        "        \"\"\"Use nearest neighbor classification\"\"\"\n",
        "        if self.nn_classifier is None:\n",
        "            return np.array([-1] * len(embeddings_reduced))\n",
        "\n",
        "        distances, indices = self.nn_classifier.kneighbors(embeddings_reduced)\n",
        "        neighbor_labels = self.nn_labels[indices]\n",
        "\n",
        "        labels = []\n",
        "        for i in range(len(embeddings_reduced)):\n",
        "            unique, counts = np.unique(neighbor_labels[i], return_counts=True)\n",
        "            if len(unique) > 0:\n",
        "                majority_label = unique[np.argmax(counts)]\n",
        "                # Check if the distance is reasonable\n",
        "                avg_distance = np.mean(distances[i])\n",
        "                if avg_distance < 1.0:\n",
        "                    labels.append(majority_label)\n",
        "                else:\n",
        "                    labels.append(-1)\n",
        "            else:\n",
        "                labels.append(-1)\n",
        "\n",
        "        labels = np.array(labels)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        print(f\" NN prediction: {n_clusters} clusters found\")\n",
        "        return labels\n",
        "\n",
        "    def _predict_centroid(self, embeddings_reduced):\n",
        "        \"\"\"Assign to nearest cluster centroid\"\"\"\n",
        "        if not self.cluster_centroids:\n",
        "            return np.array([-1] * len(embeddings_reduced))\n",
        "\n",
        "        labels = []\n",
        "        for point in embeddings_reduced:\n",
        "            min_dist = float('inf')\n",
        "            best_label = -1\n",
        "\n",
        "            for label, centroid in self.cluster_centroids.items():\n",
        "                dist = np.linalg.norm(point - centroid)\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    best_label = label\n",
        "\n",
        "            # Use distance threshold\n",
        "            if min_dist < 1.0:\n",
        "                labels.append(best_label)\n",
        "            else:\n",
        "                labels.append(-1)\n",
        "\n",
        "        labels = np.array(labels)\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        print(f\" Centroid prediction: {n_clusters} clusters found\")\n",
        "        return labels\n",
        "\n",
        "    def _predict_hybrid(self, embeddings_reduced):\n",
        "        \"\"\"Hybrid approach\"\"\"\n",
        "        print(\" Trying hybrid prediction approach...\")\n",
        "\n",
        "        # Try HDBSCAN first\n",
        "        hdbscan_labels = self._predict_hdbscan(embeddings_reduced)\n",
        "        hdbscan_coverage = np.sum(hdbscan_labels != -1) / len(hdbscan_labels)\n",
        "\n",
        "        if hdbscan_coverage > 0.5:\n",
        "            print(f\" Using HDBSCAN prediction (coverage: {hdbscan_coverage:.1%})\")\n",
        "            return hdbscan_labels\n",
        "\n",
        "        # Otherwise try NN\n",
        "        nn_labels = self._predict_nearest_neighbor(embeddings_reduced)\n",
        "        nn_coverage = np.sum(nn_labels != -1) / len(nn_labels)\n",
        "\n",
        "        if nn_coverage > hdbscan_coverage:\n",
        "            print(f\" Using NN prediction (coverage: {nn_coverage:.1%})\")\n",
        "            return nn_labels\n",
        "\n",
        "        # Finally try centroid\n",
        "        centroid_labels = self._predict_centroid(embeddings_reduced)\n",
        "        print(f\" Using centroid prediction\")\n",
        "        return centroid_labels\n",
        "\n",
        "# IMPROVED MAIN PIPELINE\n",
        "def improved_main_pipeline():\n",
        "    print(\" STARTING IMPROVED SEMANTIC EMBEDDING & CATEGORIZATION PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\" MODE: Improved clustering with meaningful categories\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # Step 0: Load data\n",
        "        print(\"\\n STEP 1: Loading Data\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        train_df = pd.read_csv(\"dataset/train.csv\")\n",
        "        test_df = pd.read_csv(\"dataset/test.csv\")\n",
        "\n",
        "        # Handle missing values\n",
        "        train_df[\"catalog_content\"] = train_df[\"catalog_content\"].fillna(\"No description\")\n",
        "        test_df[\"catalog_content\"] = test_df[\"catalog_content\"].fillna(\"No description\")\n",
        "\n",
        "        print(f\" Data loaded:\")\n",
        "        print(f\"   - Training samples: {len(train_df)}\")\n",
        "        print(f\"   - Test samples: {len(test_df)}\")\n",
        "\n",
        "        # Step 1: Extract embeddings with improved model\n",
        "        print(\"\\n STEP 2: Extracting Improved Semantic Embeddings\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        embedder = SemanticEmbedder(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "        print(\" Generating TRAIN embeddings...\")\n",
        "        train_embeddings = embedder.get_embeddings(train_df[\"catalog_content\"])\n",
        "\n",
        "        print(\" Generating TEST embeddings...\")\n",
        "        test_embeddings = embedder.get_embeddings(test_df[\"catalog_content\"])\n",
        "\n",
        "        # Save embeddings\n",
        "        save_embeddings(train_embeddings, \"improved_train_embeddings.npy\")\n",
        "        save_embeddings(test_embeddings, \"improved_test_embeddings.npy\")\n",
        "\n",
        "        # Step 2: Improved categorization\n",
        "        print(\"\\n STEP 3: Improved Automatic Categorization\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        categorizer = ImprovedAutoCategorizer(dataset_size=len(train_df))\n",
        "        train_labels = categorizer.fit(train_embeddings, train_df[\"catalog_content\"])\n",
        "\n",
        "        # Predict categories for test data\n",
        "        print(\"\\n STEP 3b: Predicting Categories for Test Data\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        test_labels = categorizer.predict(test_embeddings, method='hybrid')\n",
        "\n",
        "        # Analyze clustering quality\n",
        "        unique_train_labels = np.unique(train_labels)\n",
        "        if len(unique_train_labels) > 1 or (len(unique_train_labels) == 1 and unique_train_labels[0] != -1):\n",
        "            analyze_clustering_quality(train_embeddings, train_labels)\n",
        "        else:\n",
        "            print(\"No clusters found for quality analysis\")\n",
        "\n",
        "        # Print improved category summary\n",
        "        if categorizer.category_interpretations:\n",
        "            print_improved_category_summary(categorizer.category_interpretations, len(train_labels))\n",
        "        else:\n",
        "            print(\"No category interpretations available\")\n",
        "\n",
        "        # Save results\n",
        "        save_categories(train_labels, categorizer.category_interpretations, \"improved_train_categories.npy\")\n",
        "        save_categories(test_labels, categorizer.category_interpretations, \"improved_test_categories.npy\")\n",
        "\n",
        "        # Step 3: Visualizations\n",
        "        print(\"\\n STEP 4: Creating Visualizations\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if categorizer.embedding_2d is not None:\n",
        "            plot_clusters(categorizer.embedding_2d, train_labels, \"Improved Train Data - Product Categories\")\n",
        "\n",
        "            # Generate 2D for test data\n",
        "            umap_2d_test = umap.UMAP(n_components=2, random_state=42, metric='cosine', min_dist=0.1)\n",
        "            test_embedding_2d = umap_2d_test.fit_transform(test_embeddings)\n",
        "            plot_clusters(test_embedding_2d, test_labels, \"Improved Test Data - Product Categories\")\n",
        "\n",
        "        plot_category_sizes(train_labels, \"improved_train_category_sizes.png\")\n",
        "        plot_category_sizes(test_labels, \"improved_test_category_sizes.png\")\n",
        "\n",
        "        print(\"\\n IMPROVED PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Final statistics\n",
        "        n_train_clusters = len(set(train_labels)) - (1 if -1 in train_labels else 0)\n",
        "        n_train_noise = np.sum(train_labels == -1)\n",
        "        train_coverage = (len(train_labels) - n_train_noise) / len(train_labels) * 100\n",
        "\n",
        "        n_test_clusters = len(set(test_labels)) - (1 if -1 in test_labels else 0)\n",
        "        n_test_noise = np.sum(test_labels == -1)\n",
        "        test_coverage = (len(test_labels) - n_test_noise) / len(test_labels) * 100\n",
        "\n",
        "        print(f\"\\n IMPROVED RESULTS SUMMARY:\")\n",
        "        print(f\"   TRAIN DATA:\")\n",
        "        print(f\"     - Total products: {len(train_labels)}\")\n",
        "        print(f\"     - Categories discovered: {n_train_clusters}\")\n",
        "        print(f\"     - Noise products: {n_train_noise} ({n_train_noise/len(train_labels)*100:.1f}%)\")\n",
        "        print(f\"     - Categorized products: {len(train_labels)-n_train_noise} ({train_coverage:.1f}%)\")\n",
        "\n",
        "        print(f\"   TEST DATA:\")\n",
        "        print(f\"     - Total products: {len(test_labels)}\")\n",
        "        print(f\"     - Categories assigned: {n_test_clusters}\")\n",
        "        print(f\"     - Noise products: {n_test_noise} ({n_test_noise/len(test_labels)*100:.1f}%)\")\n",
        "        print(f\"     - Categorized products: {len(test_labels)-n_test_noise} ({test_coverage:.1f}%)\")\n",
        "\n",
        "        return categorizer, train_labels, test_labels\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n PIPELINE FAILED: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# IMPROVED ANALYSIS FUNCTIONS\n",
        "def print_improved_category_summary(interpretations, total_samples):\n",
        "    \"\"\"Print improved summary of discovered categories\"\"\"\n",
        "    if not interpretations:\n",
        "        print(\" No category interpretations available\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n MEANINGFUL CATEGORIES DISCOVERED\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Sort categories by size\n",
        "    sorted_categories = sorted(\n",
        "        interpretations.items(),\n",
        "        key=lambda x: x[1]['size'],\n",
        "        reverse=True\n",
        "    )[:15]  # Show top 15\n",
        "\n",
        "    for i, (label, info) in enumerate(sorted_categories, 1):\n",
        "        percentage = (info['size'] / total_samples) * 100\n",
        "        print(f\"{i:2d}. {info['category_name']} (Size: {info['size']:4d} products, {percentage:.1f}%)\")\n",
        "        print(f\"     Sample: {info['representative_description']}\")\n",
        "        print(f\"     Keywords: {', '.join(info['top_keywords'])}\")\n",
        "        print()\n",
        "\n",
        "# Keep the existing utility functions (save_embeddings, load_embeddings, etc.)\n",
        "\n",
        "if _name_ == \"_main_\":\n",
        "    improved_main_pipeline()"
      ],
      "metadata": {
        "id": "WLd14MWVPtnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure Feature from train_embedding.npy"
      ],
      "metadata": {
        "id": "pCSrlqD9REhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_FILE =  'train.csv'\n",
        "TEST_FILE =  'test.csv'\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    try:\n",
        "        df_train = pd.read_csv(TRAIN_FILE)\n",
        "        df_test = pd.read_csv(TEST_FILE)\n",
        "\n",
        "        return df_train.copy(), df_test.copy()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find files . Please check paths.\")\n",
        "\n",
        "\n",
        "N_brands = 0\n",
        "N_categories = 0"
      ],
      "metadata": {
        "id": "moJl_fDJRF88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Build TF-IDF representation of clusters ---\n"
      ],
      "metadata": {
        "id": "0bBOrszgR1OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "data = np.load(\"all_categories.npy\", allow_pickle=True).item()\n",
        "interpretations = data[\"interpretations\"]\n",
        "\n",
        "\n",
        "cluster_ids = list(interpretations.keys())\n",
        "cluster_texts = [\n",
        "    f\"{v['representative_description']} {' '.join(v['top_keywords'])}\"\n",
        "    for v in interpretations.values()\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "cluster_vectors = vectorizer.fit_transform(cluster_texts)\n",
        "import re\n",
        "\n",
        "def extract_brand(content: str) -> str:\n",
        "\n",
        "    # 1. Extract 'Item Name'\n",
        "    item_name_match = re.search(r'Item Name:\\s*(.*?)\\n', content, re.DOTALL)\n",
        "    if not item_name_match:\n",
        "        return 'Unknown'\n",
        "\n",
        "    item_name = item_name_match.group(1).strip()\n",
        "    if not item_name:\n",
        "        return 'Unknown'\n",
        "\n",
        "    # 2. Remove trailing punctuation/dashes\n",
        "    item_name = re.sub(r'[-—–,:].*$', '', item_name).strip()\n",
        "\n",
        "    # 3. Split into words\n",
        "    words = item_name.split()\n",
        "    if not words:\n",
        "        return 'Unknown'\n",
        "\n",
        "    # 4. Collect brand words at start\n",
        "    brand_words = []\n",
        "    stop_words = {'and', 'or', 'the', 'for', 'a', 'an'}\n",
        "\n",
        "    for word in words:\n",
        "        # Accept any alphabetic starting word (capitalized or lowercase)\n",
        "        if word[0].isalpha():\n",
        "            brand_words.append(word)\n",
        "        elif word.lower() in stop_words and len(brand_words) > 0:\n",
        "            brand_words.append(word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if not brand_words:\n",
        "        return 'Unknown'\n",
        "\n",
        "    # Remove trailing stop words\n",
        "    while brand_words and brand_words[-1].lower() in stop_words:\n",
        "        brand_words.pop()\n",
        "\n",
        "    return \" \".join(brand_words)\n",
        "\n",
        "\n",
        "def extract_ipq(content: str) -> float:\n",
        "\n",
        "\n",
        "    value_match = re.search(r'Value:\\s*(\\d+\\.?\\d*)', content, re.IGNORECASE)\n",
        "    if value_match:\n",
        "        return float(value_match.group(1))\n",
        "\n",
        "\n",
        "    unit_match = re.search(r'(\\d+\\.?\\d*)\\s*(oz|g|lb|count|pack)', content, re.IGNORECASE)\n",
        "    if unit_match:\n",
        "        return float(unit_match.group(1))\n",
        "\n",
        "    return 1.0\n",
        "\n",
        "def extract_quantity(content: str) -> float:\n",
        "\n",
        "    if not isinstance(content, str):\n",
        "        return 1.0\n",
        "\n",
        "\n",
        "    patterns = [\n",
        "        r'pack\\s*of\\s*(\\d+)',\n",
        "        r'(\\d+)\\s*-\\s*pack',\n",
        "        r'(\\d+)\\s*pack',\n",
        "        r'(\\d+)\\s*count',\n",
        "        r'x\\s*(\\d+)'\n",
        "    ]\n",
        "    for pat in patterns:\n",
        "        m = re.search(pat, content, re.IGNORECASE)\n",
        "        if m:\n",
        "            return float(m.group(1))\n",
        "    return 1.0\n",
        "\n",
        "def extract_unit_value(content: str) -> float:\n",
        "\n",
        "    if not isinstance(content, str):\n",
        "        return 0.0\n",
        "\n",
        "    # Match number + unit (oz, g, ml, etc.)\n",
        "    match = re.search(r'(\\d+\\.?\\d*)\\s*(ml|l|oz|g|kg|lb)', content, re.IGNORECASE)\n",
        "    if not match:\n",
        "        return 0.0\n",
        "\n",
        "    value, unit = float(match.group(1)), match.group(2).lower()\n",
        "\n",
        "\n",
        "    conversions = {\n",
        "        'ml': 1,\n",
        "        'l': 1000,\n",
        "        'oz': 28.35,\n",
        "        'g': 1,\n",
        "        'kg': 1000,\n",
        "        'lb': 453.59\n",
        "    }\n",
        "    return value * conversions.get(unit, 1.0)\n",
        "\n",
        "\n",
        "def extract_category(content: str) -> str:\n",
        "\n",
        "    if not content or not isinstance(content, str):\n",
        "        return \"Miscellaneous\"\n",
        "\n",
        "    text_lower = content.lower().strip()\n",
        "\n",
        "    # comprehensive list of words that are generic (nuisance)\n",
        "    NUISANCE_WORDS = {\n",
        "        'point', 'bullet', 'value', 'our', 'your', 'for', 'item', 'name',\n",
        "        'product', 'description', 'pack', 'unit', 'count', 'ounce', 'oz',\n",
        "        'fl', 'fluid', 'gallon', 'quart', 'liter', 'ml', 'g', 'kg', 'lb',\n",
        "        'can', 'bottle', 'case', 'box', 'set', 'bag', 'mix', 'style',\n",
        "        'assortment', 'free', 'natural', 'pure', 'food', 'drink', 'premium',\n",
        "        'great', 'delicious', 'gift', 'kit', 'original', 'favorite'\n",
        "    }\n",
        "\n",
        "    # Clustering and Retrieval (Tier 3: Best Match)\n",
        "\n",
        "    text_vector = vectorizer.transform([text_lower])\n",
        "    sims = cosine_similarity(text_vector, cluster_vectors).flatten()\n",
        "    best_cluster_idx = sims.argmax()\n",
        "    best_cluster_id = cluster_ids[best_cluster_idx]\n",
        "\n",
        "y\n",
        "    top_words = interpretations[best_cluster_id][\"top_keywords\"]\n",
        "\n",
        "    # 2. Tier 1: Strict Filtering (Get 3 meaningful words)\n",
        "    meaningful_words = [word for word in top_words if word.lower() not in NUISANCE_WORDS]\n",
        "\n",
        "    if len(meaningful_words) >= 1:\n",
        "        # Success: Found at least one good word. Use the top 3.\n",
        "        final_category = \" \".join(meaningful_words[:3]).title()\n",
        "    else:\n",
        "        e.\n",
        "        if top_words:\n",
        "            fallback_word = top_words[0]\n",
        "            # Double-check for extremely short words which are often abbreviations\n",
        "            if len(fallback_word) > 2:\n",
        "                final_category = fallback_word.title()\n",
        "            else:\n",
        "                final_category = \"Product - \" + fallback_word.upper()\n",
        "        else:\n",
        "\n",
        "            content_words = [word.strip() for word in text_lower.split()\n",
        "                             if word.strip() and word.lower() not in NUISANCE_WORDS]\n",
        "\n",
        "            if content_words:\n",
        "                final_category = \"Content - \" + \" \".join(content_words[:2]).title()\n",
        "            else:\n",
        "                final_category = \"Miscellaneous\"\n",
        "\n",
        "    return final_category"
      ],
      "metadata": {
        "id": "m_VKcCeARw5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- MAIN PROCESSING PIPELINE ---\n"
      ],
      "metadata": {
        "id": "qm0h_T6WR9qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def prepare_structured_features(df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
        "\n",
        "    global N_brands, N_categories\n",
        "\n",
        "    print(\"1. Extracting Brand, IPQ, Category, Quantity, and Unit...\")\n",
        "\n",
        "    for df in [df_train, df_test]:\n",
        "        df['brand'] = df['catalog_content'].apply(extract_brand)\n",
        "        df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
        "        df['category'] = df['catalog_content'].apply(extract_category)\n",
        "        df['quantity'] = df['catalog_content'].apply(extract_quantity)\n",
        "        df['unit_value'] = df['catalog_content'].apply(extract_unit_value)\n",
        "\n",
        "    print(\"2. Encoding categorical features...\")\n",
        "    brand_encoder = LabelEncoder()\n",
        "    category_encoder = LabelEncoder()\n",
        "\n",
        "    all_brands = pd.concat([df_train['brand'], df_test['brand']]).unique()\n",
        "    all_categories = pd.concat([df_train['category'], df_test['category']]).unique()\n",
        "\n",
        "    brand_encoder.fit(all_brands)\n",
        "    category_encoder.fit(all_categories)\n",
        "\n",
        "    df_train['brand_encoded'] = brand_encoder.transform(df_train['brand'])\n",
        "    df_test['brand_encoded'] = brand_encoder.transform(df_test['brand'])\n",
        "\n",
        "    df_train['category_encoded'] = category_encoder.transform(df_train['category'])\n",
        "    df_test['category_encoded'] = category_encoder.transform(df_test['category'])\n",
        "\n",
        "    N_brands = len(brand_encoder.classes_)\n",
        "    N_categories = len(category_encoder.classes_)\n",
        "\n",
        "    print(\"3. Scaling numerical features (IPQ, Quantity, Unit)...\")\n",
        "    num_scaler = StandardScaler()\n",
        "\n",
        "    # Combine numerical features\n",
        "    train_nums = df_train[['ipq', 'quantity', 'unit_value']].fillna(0)\n",
        "    test_nums = df_test[['ipq', 'quantity', 'unit_value']].fillna(0)\n",
        "\n",
        "    scaled_train = num_scaler.fit_transform(train_nums)\n",
        "    scaled_test = num_scaler.transform(test_nums)\n",
        "\n",
        "    # Split back to columns\n",
        "    df_train[['ipq_scaled', 'quantity_scaled', 'unit_scaled']] = scaled_train\n",
        "    df_test[['ipq_scaled', 'quantity_scaled', 'unit_scaled']] = scaled_test\n",
        "\n",
        "    print(f\"\\nEncoding complete. Found {N_brands} brands and {N_categories} categories.\")\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def save_structured_output(df_train: pd.DataFrame, df_test: pd.DataFrame,\n",
        "                           N_brands: int, N_categories: int):\n",
        "\n",
        "\n",
        "    # Define columns to be saved\n",
        "    feature_cols = ['brand_encoded', 'category_encoded', 'ipq_scaled', 'quantity_scaled', 'unit_scaled']\n",
        "\n",
        "\n",
        "    train_features = df_train[feature_cols].values\n",
        "    np.save('./structured_features_train.npy', train_features)\n",
        "    print(f\" Saved training structured features → structured_features_train.npy (Shape: {train_features.shape})\")\n",
        "\n",
        "    test_features = df_test[feature_cols].values\n",
        "    np.save('./structured_features_test.npy', test_features)\n",
        "    print(f\"Saved test structured features → structured_features_test.npy (Shape: {test_features.shape})\")\n",
        "\n",
        "    model_dims = {\n",
        "        \"N_brands\": N_brands,\n",
        "        \"N_categories\": N_categories,\n",
        "        \"feature_order\": feature_cols\n",
        "    }\n",
        "\n",
        "    with open('./structured_model_config.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(model_dims, f, indent=4)\n",
        "    print(\" Saved model configuration → structured_model_config.json\")\n",
        "\n",
        "\n",
        "    readable_cols = feature_cols + ['brand', 'category']\n",
        "    if all(col in df_train.columns for col in readable_cols):\n",
        "        train_records = df_train[readable_cols].to_dict(orient='records')\n",
        "        with open('./structured_train_data.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(train_records, f, ensure_ascii=False, indent=4)\n",
        "        print(f\" Saved readable structured data → structured_train_data.json ({len(train_records)} records)\")\n",
        "    else:\n",
        "        print(\" Warning: Some readable columns not found; skipping JSON export.\")\n",
        "\n",
        "    print(\"\\n All structured outputs saved successfully.\")"
      ],
      "metadata": {
        "id": "tFNqb0JVR7sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- EXECUTION BLOCK ---\n"
      ],
      "metadata": {
        "id": "3XdYSRgTSDdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    df_train, df_test = load_data()\n",
        "\n",
        "    # Run the pipeline\n",
        "    df_train_processed, df_test_processed = prepare_structured_features(df_train, df_test)\n",
        "\n",
        "    print(\"\\n--- Training Data Sample (Final Structured Inputs) ---\")\n",
        "    print(df_train_processed[['brand_encoded', 'category_encoded', 'ipq_scaled', 'brand', 'category']].head())\n",
        "\n",
        "    print(\"\\n--- Keras/TF Inputs for Structured Branch ---\")\n",
        "    print(f\"Total Unique Brands (Embedding input_dim): {N_brands}\")\n",
        "    print(f\"Total Unique Categories (Embedding input_dim): {N_categories}\")\n",
        "    print(f\"IPQ Scaled Array Shape: {df_train_processed['ipq_scaled'].values.shape}\")\n",
        "    save_structured_output(df_train_processed, df_test_processed, N_brands=N_brands, N_categories=N_categories)"
      ],
      "metadata": {
        "id": "GMvEMi4LSBwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "#  Load the file\n",
        "data = np.load(\"all_categories.npy\", allow_pickle=True)\n",
        "\n",
        "#  If it's a dictionary (like your file), extract it\n",
        "if isinstance(data.item(), dict):\n",
        "    data = data.item()\n",
        "\n",
        "#  See what keys exist\n",
        "print(\"Keys in file:\", list(data.keys()))\n",
        "\n",
        "#  Display summary for each key\n",
        "for key, value in data.items():\n",
        "    print(f\"\\n--- {key} ---\")\n",
        "    if isinstance(value, np.ndarray):\n",
        "        print(f\"Shape: {value.shape}, dtype: {value.dtype}\")\n",
        "    elif isinstance(value, dict):\n",
        "        print(f\"Dictionary with {len(value)} entries. Example:\")\n",
        "        pprint(list(value.items())[:5])\n",
        "    else:\n",
        "        print(\"Type:\", type(value))\n",
        "        print(\"Example:\", value)\n"
      ],
      "metadata": {
        "id": "Bh5OQNNHSGT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---  Now,Training Model from train.csv,train_embedding,train_strutrue_feature.npy,Train_image_feature.npy ---"
      ],
      "metadata": {
        "id": "ZZg_xZoTSLjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Pipeline: Supervised Autoencoder + Stacking Ensemble\n",
        "\n",
        "import os, gc, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load Data\n",
        "train_path = \"train.csv\"\n",
        "if not os.path.exists(train_path):\n",
        "    raise FileNotFoundError(\" train.csv not found\")\n",
        "df_train = pd.read_csv(train_path)\n",
        "print(f\" Loaded train.csv: {df_train.shape}\")\n",
        "\n",
        "#  Load / Simulate Features\n",
        "def load_or_simulate(path, shape):\n",
        "    if os.path.exists(path):\n",
        "        arr = np.load(path)\n",
        "        print(f\" Loaded {os.path.basename(path)}: {arr.shape}\")\n",
        "    else:\n",
        "        print(f\"{os.path.basename(path)} not found — simulating features\")\n",
        "        arr = np.random.rand(*shape).astype(np.float32)\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "X_structured = load_or_simulate(\"structured_features_train.npy\", (len(df_train), 5))\n",
        "X_text = load_or_simulate(\"train_embeddings.npy\", (len(df_train), 384))\n",
        "X_image = load_or_simulate(\"merged_image_features_75k.npy\", (len(df_train), 2048))\n",
        "\n",
        "#  Scale Features\n",
        "scaler_struct = StandardScaler()\n",
        "X_structured_scaled = scaler_struct.fit_transform(X_structured)\n",
        "\n",
        "scaler_text = StandardScaler()\n",
        "X_text_scaled = scaler_text.fit_transform(X_text)\n",
        "\n",
        "scaler_image = StandardScaler()\n",
        "X_image_scaled = scaler_image.fit_transform(X_image)\n",
        "\n",
        "# Supervised Autoencoder for Image Features\n",
        "X_image_tensor = torch.from_numpy(X_image_scaled).float()\n",
        "y_tensor = torch.from_numpy(np.log1p(df_train[\"price\"].values.reshape(-1,1)).astype(np.float32))\n",
        "\n",
        "class SupervisedAutoencoder(nn.Module):\n",
        "    def _init_(self, input_dim=2048, bottleneck_dim=512):\n",
        "        super()._init_()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, bottleneck_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, input_dim)\n",
        "        )\n",
        "        self.regressor = nn.Linear(bottleneck_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        y_pred = self.regressor(z)\n",
        "        return x_hat, y_pred, z\n",
        "\n",
        "bottleneck_dim = 512\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "learning_rate = 1e-3\n",
        "alpha = 0.5\n",
        "\n",
        "model = SupervisedAutoencoder(input_dim=X_image_scaled.shape[1], bottleneck_dim=bottleneck_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion_recon = nn.MSELoss()\n",
        "criterion_reg = nn.MSELoss()\n",
        "\n",
        "dataset = TensorDataset(X_image_tensor, y_tensor)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        x_hat, y_pred, _ = model(x_batch)\n",
        "        loss_recon = criterion_recon(x_hat, x_batch)\n",
        "        loss_reg = criterion_reg(y_pred, y_batch)\n",
        "        loss = alpha*loss_recon + (1-alpha)*loss_reg\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(dataset):.6f}\")\n",
        "\n",
        "# Bottleneck embeddings\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _, _, X_image_bottleneck = model(X_image_tensor)\n",
        "    X_image_bottleneck = X_image_bottleneck.numpy().astype(np.float32)\n",
        "\n",
        "print(f\" Image embeddings reduced: {X_image_scaled.shape} → {X_image_bottleneck.shape}\")\n",
        "\n",
        "# Combine all features\n",
        "X_full = np.hstack([X_structured_scaled, X_text_scaled, X_image_bottleneck]).astype(np.float32)\n",
        "y = np.log1p(df_train[\"price\"].values.astype(np.float32))\n",
        "print(f\" Combined features shape: {X_full.shape}\")\n",
        "\n",
        "# Clean up\n",
        "del X_image_scaled, X_image_tensor\n",
        "gc.collect()\n",
        "\n",
        "#  Define Base Models\n",
        "base_models = {\n",
        "    \"lgb\": lgb.LGBMRegressor(\n",
        "        n_estimators=200, num_leaves=31, learning_rate=0.05,\n",
        "        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=1, verbose=-1\n",
        "    ),\n",
        "    \"xgb\": xgb.XGBRegressor(\n",
        "        n_estimators=200, max_depth=5, learning_rate=0.05,\n",
        "        subsample=0.8, colsample_bytree=0.8, tree_method='hist',\n",
        "        random_state=42, n_jobs=1, verbosity=0\n",
        "    ),\n",
        "    \"mlp\": MLPRegressor(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation='relu',\n",
        "        batch_size=512,\n",
        "        learning_rate_init=0.001,\n",
        "        max_iter=300,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "}\n",
        "\n",
        "#  K-Fold Stacking\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "meta_features = np.zeros((len(X_full), len(base_models)), dtype=np.float32)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n",
        "    print(f\"\\n===== Fold {fold} =====\")\n",
        "    X_train, X_val = X_full[train_idx], X_full[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    for i, (name, model_) in enumerate(base_models.items()):\n",
        "        print(f\"→ Training {name.upper()}...\")\n",
        "        model_.fit(X_train, y_train)\n",
        "        preds_val = model_.predict(X_val)\n",
        "        meta_features[val_idx, i] = preds_val\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
        "        print(f\"   RMSE ({name}): {rmse:.4f}\")\n",
        "    gc.collect()\n",
        "\n",
        "#  Train Meta Model\n",
        "meta_model = lgb.LGBMRegressor(\n",
        "    n_estimators=300, learning_rate=0.03, num_leaves=31, random_state=42\n",
        ")\n",
        "meta_model.fit(meta_features, y)\n",
        "print(\"Meta model trained successfully\")\n",
        "\n",
        "#  Evaluate\n",
        "meta_preds = meta_model.predict(meta_features)\n",
        "rmse = np.sqrt(mean_squared_error(y, meta_preds))\n",
        "r2 = r2_score(y, meta_preds)\n",
        "smape_val = 100*np.mean(2*np.abs(np.expm1(meta_preds)-np.expm1(y))/(np.expm1(meta_preds)+np.expm1(y)+1e-8))\n",
        "\n",
        "print(f\"\\n Validation Results:\\nRMSE: {rmse:.4f}\\nR²: {r2:.4f}\\nSMAPE: {smape_val:.2f}%\")\n",
        "\n",
        "# Save Models\n",
        "final_model = {\n",
        "    \"scalers\": {\"structured\": scaler_struct, \"text\": scaler_text, \"image\": scaler_image},\n",
        "    \"autoencoder\": model.state_dict(),\n",
        "    \"base_models\": base_models,\n",
        "    \"meta_model\": meta_model\n",
        "}\n",
        "joblib.dump(final_model, \"stacked_model_supervised_autoencoder.pkl\")\n",
        "print(\" Models saved successfully!\")"
      ],
      "metadata": {
        "id": "587l4BgESoWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **--- Now test the model using the Stacked_model_supervised_autoencoder.pk file, test_embeddings.NPY, test.csv, and structured_test_features.npy ---**"
      ],
      "metadata": {
        "id": "XGhpxUiHVrX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -- THEN,IT TEST_OUT.CSV FILE. --"
      ],
      "metadata": {
        "id": "14xQHN0SWyKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Pipeline: Predict Product Price (Test Data)\n"
      ],
      "metadata": {
        "id": "lynOW7G1XE2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Pipeline: Predict Product Price (Test Data)\n",
        "\n",
        "import os, gc, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Trained Model and Scalers\n",
        "model_bundle = joblib.load(r'/content/stacked_model_supervised_autoencoder.pkl')\n",
        "\n",
        "scaler_struct = model_bundle[\"scalers\"][\"structured\"]\n",
        "scaler_text = model_bundle[\"scalers\"][\"text\"]\n",
        "scaler_image = model_bundle[\"scalers\"][\"image\"]\n",
        "\n",
        "autoencoder_state = model_bundle[\"autoencoder\"]\n",
        "base_models = model_bundle[\"base_models\"]\n",
        "meta_model = model_bundle[\"meta_model\"]\n",
        "\n",
        "print(\" Loaded all models successfully!\")\n",
        "\n",
        "#  Define Autoencoder Class (same as used in training)\n",
        "class SupervisedAutoencoder(nn.Module):\n",
        "    def _init_(self, input_dim=2048, bottleneck_dim=512):\n",
        "        super()._init_()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, bottleneck_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, input_dim)\n",
        "        )\n",
        "        self.regressor = nn.Linear(bottleneck_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        y_pred = self.regressor(z)\n",
        "        return x_hat, y_pred, z\n",
        "\n",
        "# Recreate the model and load trained weights\n",
        "autoencoder = SupervisedAutoencoder(input_dim=2048, bottleneck_dim=512)\n",
        "autoencoder.load_state_dict(autoencoder_state)\n",
        "autoencoder.eval()\n",
        "\n",
        "print(\"Autoencoder loaded and ready!\")\n",
        "\n",
        "#  Load Test Dataset\n",
        "test_path = r'test.csv'\n",
        "if not os.path.exists(test_path):\n",
        "    raise FileNotFoundError(\" test.csv not found\")\n",
        "\n",
        "df_test = pd.read_csv(test_path)\n",
        "print(f\" Loaded test.csv: {df_test.shape}\")\n",
        "\n",
        "#  Load Test Features\n",
        "def load_or_simulate(path, shape):\n",
        "    if os.path.exists(path):\n",
        "        arr = np.load(path)\n",
        "        print(f\" Loaded {os.path.basename(path)}: {arr.shape}\")\n",
        "    else:\n",
        "        print(f\" {os.path.basename(path)} not found — simulating features\")\n",
        "        arr = np.random.rand(*shape).astype(np.float32)\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "X_structured_test = load_or_simulate(r'/content/drive/MyDrive/amazon/structured_features_test.npy', (len(df_test), 5))\n",
        "X_text_test = load_or_simulate(r'/content/drive/MyDrive/amazon/test_embeddings.npy', (len(df_test), 384))\n",
        "X_image_test = load_or_simulate(r'/content/Test_merged_image_features_75k.npy', (len(df_test), 2048))\n",
        "\n",
        "#  Scale Test Features (using training scalers)\n",
        "X_structured_scaled = scaler_struct.transform(X_structured_test)\n",
        "X_text_scaled = scaler_text.transform(X_text_test)\n",
        "X_image_scaled = scaler_image.transform(X_image_test)\n",
        "\n",
        "#  Encode Image Features via Autoencoder (bottleneck)\n",
        "X_image_tensor = torch.from_numpy(X_image_scaled).float()\n",
        "with torch.no_grad():\n",
        "    _, _, X_image_bottleneck = autoencoder(X_image_tensor)\n",
        "    X_image_bottleneck = X_image_bottleneck.numpy().astype(np.float32)\n",
        "\n",
        "print(f\"Image bottleneck embeddings: {X_image_bottleneck.shape}\")\n",
        "\n",
        "\n",
        "#  Combine All Features\n",
        "X_test_full = np.hstack([X_structured_scaled, X_text_scaled, X_image_bottleneck]).astype(np.float32)\n",
        "print(f\" Final combined test features: {X_test_full.shape}\")\n",
        "\n",
        "# Predict Using Base Models → Generate Meta Features\n",
        "\n",
        "meta_features_test = np.zeros((len(X_test_full), len(base_models)), dtype=np.float32)\n",
        "\n",
        "for i, (name, model_) in enumerate(base_models.items()):\n",
        "    print(f\"→ Predicting with {name.upper()}...\")\n",
        "    preds_test = model_.predict(X_test_full)\n",
        "    meta_features_test[:, i] = preds_test\n",
        "\n",
        "print(f\" Meta-level features created: {meta_features_test.shape}\")\n",
        "\n",
        "\n",
        "#  Predict Final Prices Using Meta Model\n",
        "\n",
        "meta_preds_test = meta_model.predict(meta_features_test)\n",
        "final_price_preds = np.expm1(meta_preds_test)\n",
        "final_price_preds = np.maximum(final_price_preds, 0)\n",
        "\n",
        "# Save Output File\n",
        "submission = pd.DataFrame({\n",
        "    \"sample_id\": df_test[\"sample_id\"],\n",
        "    \"price\": final_price_preds\n",
        "})\n",
        "\n",
        "submission.to_csv(\"Super_test_out.csv\", index=False)\n",
        "print(\" test_out.csv saved successfully!\")\n",
        "print(submission.head())"
      ],
      "metadata": {
        "id": "s3GL4tvEWfZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}